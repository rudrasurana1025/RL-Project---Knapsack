{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f0fab35",
   "metadata": {},
   "source": [
    "# Training model for the knapsack problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af676fd3",
   "metadata": {},
   "source": [
    "Please follow the instructions given in the readme.md file and then carry over here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa623727",
   "metadata": {},
   "source": [
    "### Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad45e85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Type\n",
    "import numpy as np\n",
    "import or_gym\n",
    "import gym\n",
    "from gym import ObservationWrapper, spaces, logger\n",
    "from or_gym.envs.classic_or.knapsack import BoundedKnapsackEnv\n",
    "from stable_baselines3 import A2C, PPO\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.on_policy_algorithm import OnPolicyAlgorithm\n",
    "from torch import nn\n",
    "from gym.utils import seeding\n",
    "from or_gym.utils import assign_env_config\n",
    "import copy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640e8797",
   "metadata": {},
   "source": [
    "#### Setting seed for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca224688",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a6914e",
   "metadata": {},
   "source": [
    "#### Specifying KP maximum weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25981d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_WEIGHT = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5791eee",
   "metadata": {},
   "source": [
    "### Custom wrapper to normalize environment observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e08d395",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizingWrapper(ObservationWrapper):\n",
    "    \"\"\"Environment wrapper to divide observations by their maximum value\"\"\"\n",
    "\n",
    "    def __init__(self, env: BoundedKnapsackEnv):\n",
    "        \"\"\"Change observation space of wrapped environment\n",
    "\n",
    "        Args:\n",
    "            env (BoundedKnapsackEnv): environment to wrap (tailed towards or_gym)\n",
    "        \"\"\"\n",
    "        # Perform default wrapper initialization\n",
    "        super().__init__(env)\n",
    "        self.N=env.N\n",
    "        self.max_weight=env.max_weight\n",
    "        # Change default observation space to concatenate the three vectors from the\n",
    "        # default or_gym implementation, and allow for floating point values in the\n",
    "        # range [0, 1].\n",
    "        \n",
    "        self.observation_space = spaces.Box(0, 1, shape=(3+3*env.N,), dtype=np.float32)\n",
    "\n",
    "    def observation(self, observation: np.array):\n",
    "        \"\"\"Perform postprocessing on observations emitted by wrapped environment\n",
    "\n",
    "        Args:\n",
    "            observation (np.array): observation emitted by knapsack environment\n",
    "\n",
    "        Returns:\n",
    "            np.array: transformed observation\n",
    "        \"\"\"\n",
    "        # Convert observation to float to allow division and float output type\n",
    "        observation = observation.astype(np.float32)\n",
    "\n",
    "        # Normalize item weights\n",
    "        observation[0, :-1] = observation[0, :-1] / np.max(observation[0, :-1])\n",
    "        # Fix max weight input to 0\n",
    "        observation[0, -1] = 0.0\n",
    "        # Normalize item values\n",
    "        observation[1, :-1] = observation[1, :-1] / np.max(observation[1, :-1])\n",
    "        # Normalize current weight\n",
    "        observation[1, -1] = observation[1, -1] / self.max_weight\n",
    "        # Normalize item limits\n",
    "        observation[2, :] = observation[2, :] / np.max(observation[2, :])\n",
    "\n",
    "        # Concatenate three vectors emitted by default bounded knapsack environment\n",
    "        observation = np.reshape(observation, (3+3*self.N,))\n",
    "\n",
    "        return observation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911a3765",
   "metadata": {},
   "source": [
    "### Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "504ba485",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    algorithm_class: Type[OnPolicyAlgorithm] = PPO,\n",
    "    gamma: float = 0.99,\n",
    "    learning_rate: float = 0.0003,\n",
    "    normalize_env: bool = True,\n",
    "    activation_fn: Type[nn.Module] = nn.ReLU,\n",
    "    net_arch=[256, 256],\n",
    "    total_timesteps: int = 100000,\n",
    "    verbose: int = 1,\n",
    ") -> OnPolicyAlgorithm:\n",
    "    \"\"\"Train model with logging and checkpointing\n",
    "\n",
    "    Args:\n",
    "        algorithm_class (Type[OnPolicyAlgorithm], optional): algorithm class to use.\n",
    "            Defaults to PPO.\n",
    "        gamma (float, optional): discount factor to use.\n",
    "            Defaults to 0.99.\n",
    "        learning_rate (float, optional): learning rate to use.\n",
    "            Defaults to 0.0003.\n",
    "        normalize_env (bool, optional): whether to normalize the observation space.\n",
    "            Defaults to True.\n",
    "        activation_fn (Type[nn.Module], optional): activation function to use.\n",
    "            Defaults to nn.ReLU.\n",
    "        net_arch (list, optional): shared layer sizes for MLPPolicy.\n",
    "            Defaults to [256, 256].\n",
    "        total_timesteps (int, optional): total timesteps to train for.\n",
    "            Defaults to 150000.\n",
    "        verbose (int, optional): whether to do extensive logging.\n",
    "            Defaults to 1.\n",
    "\n",
    "    Returns:\n",
    "        OnPolicyAlgorithm: trained model\n",
    "    \"\"\"\n",
    "    # Make environment and apply normalization wrapper if specified\n",
    "    env: BoundedKnapsackEnv = or_gym.make(\n",
    "        \"Knapsack-v2\", max_weight=MAX_WEIGHT, mask=False\n",
    "    )\n",
    "    if normalize_env:\n",
    "        env = NormalizingWrapper(env)\n",
    "\n",
    "    # Initialize environment by resetting\n",
    "    env.reset()\n",
    "   \n",
    "    # Model definition\n",
    "    model = algorithm_class(\n",
    "        policy=\"MlpPolicy\",\n",
    "        env=env,\n",
    "        gamma=gamma,\n",
    "        learning_rate=learning_rate,\n",
    "        policy_kwargs=dict(\n",
    "            activation_fn=activation_fn,\n",
    "            net_arch=net_arch,\n",
    "        ),\n",
    "       \n",
    "        verbose=verbose,\n",
    "    )\n",
    "\n",
    "    # Model training\n",
    "    model.learn(\n",
    "        total_timesteps=total_timesteps,\n",
    "        \n",
    "    )\n",
    "\n",
    "    # Stop environment\n",
    "    env.close()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06756f69",
   "metadata": {},
   "source": [
    "### Defining the Model object and saving the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6354f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train_model()\n",
    "model.save(\"ppo_model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlPython3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
