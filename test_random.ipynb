{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee177c86",
   "metadata": {},
   "source": [
    "# Random testing the trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74360e6",
   "metadata": {},
   "source": [
    "### Importing the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e529db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces, logger\n",
    "from gym.utils import seeding\n",
    "from or_gym.utils import assign_env_config\n",
    "import copy\n",
    "from stable_baselines3 import PPO\n",
    "import numpy as np\n",
    "from train import NormalizingWrapper\n",
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4c86cb",
   "metadata": {},
   "source": [
    "## Unbounded Knapsack Problem\n",
    "\n",
    "    The Knapsack Problem (KP) is a combinatorial optimization problem which\n",
    "    requires the user to select from a range of goods of different values and\n",
    "    weights in order to maximize the value of the selected items within a \n",
    "    given weight limit. This version is unbounded meaning that we can select\n",
    "    items without limit. \n",
    "\n",
    "    The episodes proceed by selecting items and placing them into the\n",
    "    knapsack one at a time until the weight limit is reached or exceeded, at\n",
    "    which point the episode ends.\n",
    "\n",
    "    Observation:\n",
    "        Type: Tuple, Discrete\n",
    "        0: list of item weights\n",
    "        1: list of item values\n",
    "        2: maximum weight of the knapsack\n",
    "        3: current weight in knapsack\n",
    "\n",
    "    Actions:\n",
    "        Type: Discrete\n",
    "        0: Place item 0 into knapsack\n",
    "        1: Place item 1 into knapsack\n",
    "        2: ...\n",
    "\n",
    "    Reward:\n",
    "        Value of item successfully placed into knapsack or 0 if the item\n",
    "        doesn't fit, at which point the episode ends.\n",
    "\n",
    "    Starting State:\n",
    "        Lists of available items and empty knapsack.\n",
    "\n",
    "    Episode Termination:\n",
    "        Full knapsack or selection that puts the knapsack over the limit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01574d62",
   "metadata": {},
   "source": [
    "### Creating a environment class for this problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75438785",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KnapsackEnv(gym.Env):\n",
    "    \n",
    "    # Internal list of placed items for better rendering\n",
    "    collected_items = []\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        # Generate data with consistent random seed to ensure reproducibility\n",
    "        self.N = 200\n",
    "        self.max_weight = 200\n",
    "        self.current_weight = 0\n",
    "        self._max_reward = 10000\n",
    "        self.mask = True\n",
    "        self.seed = 0\n",
    "        self.item_numbers = np.arange(self.N)\n",
    "        self.item_weights = np.random.randint(1, 100, size=self.N)\n",
    "        self.item_values = np.random.randint(0, 100, size=self.N)\n",
    "        self.over_packed_penalty = 0\n",
    "        self.randomize_params_on_reset = False\n",
    "        self.collected_items.clear()\n",
    "        # Add env_config, if any\n",
    "        assign_env_config(self, kwargs)\n",
    "        self.set_seed()\n",
    "\n",
    "        obs_space = spaces.Box(\n",
    "            0, self.max_weight, shape=(2*self.N + 1,), dtype=np.int32)\n",
    "        self.action_space = spaces.Discrete(self.N)\n",
    "        if self.mask:\n",
    "            self.observation_space = spaces.Dict({\n",
    "                \"action_mask\": spaces.Box(0, 1, shape=(self.N,), dtype=np.uint8),\n",
    "                \"avail_actions\": spaces.Box(0, 1, shape=(self.N,), dtype=np.uint8),\n",
    "                \"state\": obs_space\n",
    "                })\n",
    "        else:\n",
    "            self.observation_space = spaces.Box(\n",
    "                0, self.max_weight, shape=(2, self.N + 1), dtype=np.int32)\n",
    "        \n",
    "        self.reset()\n",
    "\n",
    "    def sample_action(self):\n",
    "        return np.random.choice(self.item_numbers)\n",
    "\n",
    "    def set_seed(self, seed=None):\n",
    "        if seed == None:\n",
    "            seed = np.random.randint(0, np.iinfo(np.int32).max)        \n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def reset(self):\n",
    "        return self._RESET()\n",
    "\n",
    "    def step(self, action):\n",
    "        return self._STEP(action)\n",
    "        \n",
    "    def render(self):\n",
    "        total_value = 0\n",
    "        total_weight = 0\n",
    "        for i in range(self.N) :\n",
    "            if i in self.collected_items :\n",
    "                total_value += self.item_values[i]\n",
    "                total_weight += self.item_weights[i]\n",
    "        print(self.collected_items, total_value, total_weight)\n",
    "        \n",
    "        # RlLib requirement: Make sure you either return a uint8/w x h x 3 (RGB) image or handle rendering in a window and then return `True`.\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c63d416",
   "metadata": {},
   "source": [
    "## Bounded Knapsack Problem\n",
    "\n",
    "    The Knapsack Problem (KP) is a combinatorial optimization problem which\n",
    "    requires the user to select from a range of goods of different values and\n",
    "    weights in order to maximize the value of the selected items within a \n",
    "    given weight limit. This version is bounded meaning each item can be\n",
    "    selected a limited number of times.\n",
    "\n",
    "    The episodes proceed by selecting items and placing them into the\n",
    "    knapsack one at a time until the weight limit is reached or exceeded, at\n",
    "    which point the episode ends.\n",
    "\n",
    "    Observation:\n",
    "        Type: Tuple, Discrete\n",
    "        0: list of item weights\n",
    "        1: list of item values\n",
    "        2: list of item limits\n",
    "        3: maximum weight of the knapsack\n",
    "        4: current weight in knapsack\n",
    "\n",
    "    Actions:\n",
    "        Type: Discrete\n",
    "        0: Place item 0 into knapsack\n",
    "        1: Place item 1 into knapsack\n",
    "        2: ...\n",
    "\n",
    "    Reward:\n",
    "        Value of item successfully placed into knapsack or 0 if the item\n",
    "        doesn't fit, at which point the episode ends.\n",
    "\n",
    "    Starting State:\n",
    "        Lists of available items and empty knapsack.\n",
    "\n",
    "    Episode Termination:\n",
    "        Full knapsack or selection that puts the knapsack over the limit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4904891f",
   "metadata": {},
   "source": [
    "### Creating an environment class for Bounded Knapsack probelm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ce7990",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoundedKnapsackEnv(KnapsackEnv):\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.N = 200\n",
    "        self.item_limits_init = np.random.randint(1, 10, size=self.N, dtype=np.int32)\n",
    "        self.item_limits = self.item_limits_init.copy()\n",
    "        super().__init__()\n",
    "        self.item_weights = np.random.randint(1, 100, size=self.N, dtype=np.int32)\n",
    "        self.item_values = np.random.randint(0, 100, size=self.N, dtype=np.int32)\n",
    "\n",
    "        assign_env_config(self, kwargs)\n",
    "\n",
    "        obs_space = spaces.Box(\n",
    "            0, self.max_weight, shape=(3, self.N + 1), dtype=np.int32)\n",
    "        if self.mask:\n",
    "            self.observation_space = spaces.Dict({\n",
    "                \"action_mask\": spaces.Box(0, 1, shape=(len(self.item_limits),), dtype=np.uint8),\n",
    "                \"avail_actions\": spaces.Box(0, 1, shape=(len(self.item_limits),), dtype=np.uint8),\n",
    "                \"state\": obs_space\n",
    "            })\n",
    "        else:\n",
    "            self.observation_space = obs_space\n",
    "        \n",
    "    def _STEP(self, item):\n",
    "        # Check item limit\n",
    "        if self.item_limits[item] > 0:\n",
    "            # Check that item will fit\n",
    "            if self.item_weights[item] + self.current_weight <= self.max_weight:\n",
    "                self.current_weight += self.item_weights[item]\n",
    "                reward = self.item_values[item]\n",
    "                if self.current_weight == self.max_weight:\n",
    "                    done = True\n",
    "                else:\n",
    "                    done = False\n",
    "                self._update_state(item)\n",
    "            else:\n",
    "                # End if over weight\n",
    "                reward = 0\n",
    "                done = True\n",
    "        else:\n",
    "            # End if item is unavailable\n",
    "            reward = 0\n",
    "            done = True\n",
    "            \n",
    "        return self.state, reward, done, {}\n",
    "\n",
    "    def _update_state(self, item=None):\n",
    "        if item is not None:\n",
    "            self.item_limits[item] -= 1\n",
    "        state_items = np.vstack([\n",
    "            self.item_weights,\n",
    "            self.item_values,\n",
    "            self.item_limits\n",
    "        ], dtype=np.int32)\n",
    "        state = np.hstack([\n",
    "            state_items, \n",
    "            np.array([[self.max_weight],\n",
    "                      [self.current_weight], \n",
    "                      [0] # Serves as place holder\n",
    "                ], dtype=np.int32)\n",
    "        ])\n",
    "        if self.mask:\n",
    "            mask = np.where(self.current_weight + self.item_weights > self.max_weight, 0, 1).astype(np.uint8)\n",
    "            mask = np.where(self.item_limits > 0, mask, 0)\n",
    "            self.state = {\n",
    "                \"action_mask\": mask,\n",
    "                \"avail_actions\": np.ones(self.N, dtype=np.uint8),\n",
    "                \"state\": state\n",
    "            }\n",
    "        else:\n",
    "            self.state = state.copy()\n",
    "        \n",
    "    def sample_action(self):\n",
    "        return np.random.choice(\n",
    "            self.item_numbers[np.where(self.item_limits!=0)])\n",
    "    \n",
    "    def _RESET(self):\n",
    "        if self.randomize_params_on_reset:\n",
    "            self.item_weights = np.random.randint(1, 100, size=self.N, dtype=np.int32)\n",
    "            self.item_values = np.random.randint(0, 100, size=self.N, dtype=np.int32)\n",
    "            self.item_limits = np.random.randint(1, 10, size=self.N, dtype=np.int32)\n",
    "        else:\n",
    "            self.item_limits = self.item_limits_init.copy()\n",
    "\n",
    "        self.current_weight = 0\n",
    "        self._update_state()\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87bec0e",
   "metadata": {},
   "source": [
    "##### Defining the update_state, STEP, RESET, and render methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2e4ad59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_state(env, item=None):\n",
    "    if item is not None:\n",
    "        env.item_limits[item] -= 1\n",
    "        env.collected_items.append(item)\n",
    "    state_items = np.vstack([\n",
    "        env.item_weights,\n",
    "        env.item_values,\n",
    "        env.item_limits\n",
    "    ], dtype=np.int32)\n",
    "    state = np.hstack([\n",
    "        state_items, \n",
    "        np.array([[env.max_weight],\n",
    "                    [env.current_weight], \n",
    "                    [0] # Serves as place holder\n",
    "            ], dtype=np.int32)\n",
    "    ])\n",
    "    if env.mask:\n",
    "        mask = np.where(env.current_weight + env.item_weights > env.max_weight, 0, 1).astype(np.uint8)\n",
    "        mask = np.where(env.item_limits > 0, mask, 0)\n",
    "        env.state = {\n",
    "            \"action_mask\": mask,\n",
    "            \"avail_actions\": np.ones(env.N, dtype=np.uint8),\n",
    "            \"state\": state\n",
    "        }\n",
    "    else:\n",
    "        env.state = state.copy()\n",
    "\n",
    "def STEP(env,item):\n",
    "    if env.item_limits[item] > 0:\n",
    "            # Check that item will fit\n",
    "            if env.item_weights[item] + env.current_weight <= env.max_weight:\n",
    "                env.current_weight += env.item_weights[item]\n",
    "                reward = env.item_values[item]\n",
    "                if env.current_weight == env.max_weight:\n",
    "                    done = True\n",
    "                else:\n",
    "                    done = False\n",
    "                update_state(env,item)\n",
    "            else:\n",
    "                # End if over weight\n",
    "                reward = 0\n",
    "                done = True\n",
    "    else:\n",
    "        # End if item is unavailable\n",
    "        reward = 0\n",
    "        done = True\n",
    "        \n",
    "    return env.state, reward, done, {}\n",
    "\n",
    "\n",
    "def render(env):\n",
    "    total_value = 0\n",
    "    total_weight = 0\n",
    "    for i in range(env.N) :\n",
    "        if i in env.collected_items :\n",
    "            total_value += env.item_values[i]\n",
    "            total_weight += env.item_weights[i]\n",
    "    print(env.collected_items, total_value, total_weight)\n",
    "    pass\n",
    "\n",
    "\n",
    "def RESET(env):\n",
    "    if env.randomize_params_on_reset:\n",
    "        env.item_weights = np.random.randint(1, 100, size=env.N, dtype=np.int32)\n",
    "        env.item_values = np.random.randint(0, 100, size=env.N, dtype=np.int32)\n",
    "        env.item_limits = np.random.randint(1, 10, size=env.N, dtype=np.int32)\n",
    "    else:\n",
    "        env.item_limits = env.item_limits_init.copy()\n",
    "\n",
    "    env.current_weight = 0\n",
    "    update_state(env)\n",
    "    return env.state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b15000b",
   "metadata": {},
   "source": [
    "#### Load the model and create the environment instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6178ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO.load(\"ppo_model\")\n",
    "\n",
    "env = BoundedKnapsackEnv(max_weight=300, mask=False)\n",
    "\n",
    "env = NormalizingWrapper(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80341691",
   "metadata": {},
   "source": [
    "#### Defining the policy over which the Model works for 100 and 1000 episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df0b87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_reward, std_reward = evaluate_policy(\n",
    "        model=model, \n",
    "        env=env, \n",
    "        n_eval_episodes=100, \n",
    "        deterministic=False\n",
    "    )\n",
    "print(f\"Mean reward over 100 episodes: {mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "\n",
    "mean_reward, std_reward = evaluate_policy(\n",
    "        model=model, \n",
    "        env=env, \n",
    "        n_eval_episodes=1000, \n",
    "        deterministic=False\n",
    "    )\n",
    "print(f\"Mean reward over 1000 episodes: {mean_reward:.2f} +/- {std_reward:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742f30b9",
   "metadata": {},
   "source": [
    "#### Render the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e58756",
   "metadata": {},
   "outputs": [],
   "source": [
    "render(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b779a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = RESET(env)\n",
    "obs = np.reshape(obs, (603,))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeba8319",
   "metadata": {},
   "outputs": [],
   "source": [
    "states=None\n",
    "i=0\n",
    "while i<300:\n",
    "    action, states = model.predict(obs,states)\n",
    "    obs, rewards, dones, info = STEP(env,action)\n",
    "    print(rewards)\n",
    "    obs = np.reshape(obs, (603,)) \n",
    "    i+=1\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d4c717",
   "metadata": {},
   "outputs": [],
   "source": [
    "render(env)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "knapsackrl",
   "language": "python",
   "name": "knapsackrl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
